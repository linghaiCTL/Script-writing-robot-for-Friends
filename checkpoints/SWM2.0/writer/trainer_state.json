{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 1320,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.07575757575757576,
      "grad_norm": 1.864912509918213,
      "learning_rate": 9.924242424242425e-05,
      "loss": 2.4003,
      "step": 10
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 1.895506501197815,
      "learning_rate": 9.848484848484849e-05,
      "loss": 2.2019,
      "step": 20
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 4.309602737426758,
      "learning_rate": 9.772727272727274e-05,
      "loss": 2.0294,
      "step": 30
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 1.5924211740493774,
      "learning_rate": 9.696969696969698e-05,
      "loss": 2.0909,
      "step": 40
    },
    {
      "epoch": 0.3787878787878788,
      "grad_norm": 1.6104592084884644,
      "learning_rate": 9.621212121212123e-05,
      "loss": 2.0227,
      "step": 50
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 2.179314136505127,
      "learning_rate": 9.545454545454546e-05,
      "loss": 2.125,
      "step": 60
    },
    {
      "epoch": 0.5303030303030303,
      "grad_norm": 1.498256802558899,
      "learning_rate": 9.469696969696971e-05,
      "loss": 2.0364,
      "step": 70
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 1.649092674255371,
      "learning_rate": 9.393939393939395e-05,
      "loss": 1.9764,
      "step": 80
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 1.5825237035751343,
      "learning_rate": 9.318181818181818e-05,
      "loss": 2.01,
      "step": 90
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 1.6353869438171387,
      "learning_rate": 9.242424242424242e-05,
      "loss": 1.9633,
      "step": 100
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1.753942847251892,
      "learning_rate": 9.166666666666667e-05,
      "loss": 2.0225,
      "step": 110
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 2.0622498989105225,
      "learning_rate": 9.090909090909092e-05,
      "loss": 1.9388,
      "step": 120
    },
    {
      "epoch": 0.9848484848484849,
      "grad_norm": 1.725057601928711,
      "learning_rate": 9.015151515151515e-05,
      "loss": 2.0324,
      "step": 130
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 2.052288055419922,
      "learning_rate": 8.93939393939394e-05,
      "loss": 1.7918,
      "step": 140
    },
    {
      "epoch": 1.1363636363636362,
      "grad_norm": 2.4971675872802734,
      "learning_rate": 8.863636363636364e-05,
      "loss": 1.8107,
      "step": 150
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 1.9100286960601807,
      "learning_rate": 8.787878787878789e-05,
      "loss": 1.8289,
      "step": 160
    },
    {
      "epoch": 1.2878787878787878,
      "grad_norm": 2.043140411376953,
      "learning_rate": 8.712121212121212e-05,
      "loss": 1.84,
      "step": 170
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 2.3013057708740234,
      "learning_rate": 8.636363636363637e-05,
      "loss": 1.8834,
      "step": 180
    },
    {
      "epoch": 1.4393939393939394,
      "grad_norm": 2.255754232406616,
      "learning_rate": 8.560606060606061e-05,
      "loss": 1.7984,
      "step": 190
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 2.7246203422546387,
      "learning_rate": 8.484848484848486e-05,
      "loss": 1.7918,
      "step": 200
    },
    {
      "epoch": 1.5909090909090908,
      "grad_norm": 3.0363142490386963,
      "learning_rate": 8.40909090909091e-05,
      "loss": 1.8544,
      "step": 210
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 2.3039064407348633,
      "learning_rate": 8.333333333333334e-05,
      "loss": 1.7957,
      "step": 220
    },
    {
      "epoch": 1.7424242424242424,
      "grad_norm": 2.5309152603149414,
      "learning_rate": 8.257575757575758e-05,
      "loss": 1.7787,
      "step": 230
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 2.5004329681396484,
      "learning_rate": 8.181818181818183e-05,
      "loss": 1.8099,
      "step": 240
    },
    {
      "epoch": 1.893939393939394,
      "grad_norm": 3.2510666847229004,
      "learning_rate": 8.106060606060607e-05,
      "loss": 1.8214,
      "step": 250
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 2.9762203693389893,
      "learning_rate": 8.03030303030303e-05,
      "loss": 1.7685,
      "step": 260
    },
    {
      "epoch": 2.0454545454545454,
      "grad_norm": 2.282292366027832,
      "learning_rate": 7.954545454545455e-05,
      "loss": 1.6322,
      "step": 270
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 3.377769708633423,
      "learning_rate": 7.878787878787879e-05,
      "loss": 1.5893,
      "step": 280
    },
    {
      "epoch": 2.196969696969697,
      "grad_norm": 2.7038047313690186,
      "learning_rate": 7.803030303030304e-05,
      "loss": 1.5882,
      "step": 290
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 3.2605040073394775,
      "learning_rate": 7.727272727272727e-05,
      "loss": 1.5805,
      "step": 300
    },
    {
      "epoch": 2.3484848484848486,
      "grad_norm": 3.2112417221069336,
      "learning_rate": 7.651515151515152e-05,
      "loss": 1.5229,
      "step": 310
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 3.233433723449707,
      "learning_rate": 7.575757575757576e-05,
      "loss": 1.5441,
      "step": 320
    },
    {
      "epoch": 2.5,
      "grad_norm": 5.249015808105469,
      "learning_rate": 7.500000000000001e-05,
      "loss": 1.6698,
      "step": 330
    },
    {
      "epoch": 2.5757575757575757,
      "grad_norm": 2.9648170471191406,
      "learning_rate": 7.424242424242424e-05,
      "loss": 1.6434,
      "step": 340
    },
    {
      "epoch": 2.6515151515151514,
      "grad_norm": 3.593762159347534,
      "learning_rate": 7.348484848484849e-05,
      "loss": 1.6448,
      "step": 350
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 3.358748197555542,
      "learning_rate": 7.272727272727273e-05,
      "loss": 1.5735,
      "step": 360
    },
    {
      "epoch": 2.8030303030303028,
      "grad_norm": 3.7248194217681885,
      "learning_rate": 7.196969696969698e-05,
      "loss": 1.6052,
      "step": 370
    },
    {
      "epoch": 2.878787878787879,
      "grad_norm": 3.2598533630371094,
      "learning_rate": 7.121212121212121e-05,
      "loss": 1.6494,
      "step": 380
    },
    {
      "epoch": 2.9545454545454546,
      "grad_norm": 4.660173416137695,
      "learning_rate": 7.045454545454546e-05,
      "loss": 1.6539,
      "step": 390
    },
    {
      "epoch": 3.0303030303030303,
      "grad_norm": 3.4194886684417725,
      "learning_rate": 6.96969696969697e-05,
      "loss": 1.5612,
      "step": 400
    },
    {
      "epoch": 3.106060606060606,
      "grad_norm": 4.495877265930176,
      "learning_rate": 6.893939393939395e-05,
      "loss": 1.4367,
      "step": 410
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 4.208409786224365,
      "learning_rate": 6.818181818181818e-05,
      "loss": 1.4079,
      "step": 420
    },
    {
      "epoch": 3.257575757575758,
      "grad_norm": 4.640305042266846,
      "learning_rate": 6.742424242424242e-05,
      "loss": 1.3183,
      "step": 430
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 4.278496265411377,
      "learning_rate": 6.666666666666667e-05,
      "loss": 1.4292,
      "step": 440
    },
    {
      "epoch": 3.409090909090909,
      "grad_norm": 4.076400279998779,
      "learning_rate": 6.59090909090909e-05,
      "loss": 1.3965,
      "step": 450
    },
    {
      "epoch": 3.484848484848485,
      "grad_norm": 3.986692428588867,
      "learning_rate": 6.515151515151516e-05,
      "loss": 1.377,
      "step": 460
    },
    {
      "epoch": 3.5606060606060606,
      "grad_norm": 3.879894733428955,
      "learning_rate": 6.439393939393939e-05,
      "loss": 1.3887,
      "step": 470
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 5.215640068054199,
      "learning_rate": 6.363636363636364e-05,
      "loss": 1.3579,
      "step": 480
    },
    {
      "epoch": 3.712121212121212,
      "grad_norm": 5.0706377029418945,
      "learning_rate": 6.287878787878788e-05,
      "loss": 1.3853,
      "step": 490
    },
    {
      "epoch": 3.787878787878788,
      "grad_norm": 4.256466388702393,
      "learning_rate": 6.212121212121213e-05,
      "loss": 1.3565,
      "step": 500
    },
    {
      "epoch": 3.8636363636363638,
      "grad_norm": 4.46406364440918,
      "learning_rate": 6.136363636363636e-05,
      "loss": 1.4337,
      "step": 510
    },
    {
      "epoch": 3.9393939393939394,
      "grad_norm": 3.6835074424743652,
      "learning_rate": 6.060606060606061e-05,
      "loss": 1.3448,
      "step": 520
    },
    {
      "epoch": 4.015151515151516,
      "grad_norm": 4.471146106719971,
      "learning_rate": 5.9848484848484854e-05,
      "loss": 1.2803,
      "step": 530
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 6.451285362243652,
      "learning_rate": 5.90909090909091e-05,
      "loss": 1.0955,
      "step": 540
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 5.457540512084961,
      "learning_rate": 5.833333333333334e-05,
      "loss": 1.1453,
      "step": 550
    },
    {
      "epoch": 4.242424242424242,
      "grad_norm": 5.191797256469727,
      "learning_rate": 5.757575757575758e-05,
      "loss": 1.2463,
      "step": 560
    },
    {
      "epoch": 4.318181818181818,
      "grad_norm": 4.872657299041748,
      "learning_rate": 5.6818181818181825e-05,
      "loss": 1.2205,
      "step": 570
    },
    {
      "epoch": 4.393939393939394,
      "grad_norm": 6.442546844482422,
      "learning_rate": 5.606060606060606e-05,
      "loss": 1.084,
      "step": 580
    },
    {
      "epoch": 4.46969696969697,
      "grad_norm": 5.071981906890869,
      "learning_rate": 5.5303030303030304e-05,
      "loss": 1.1734,
      "step": 590
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 5.159916877746582,
      "learning_rate": 5.4545454545454546e-05,
      "loss": 1.2804,
      "step": 600
    },
    {
      "epoch": 4.621212121212121,
      "grad_norm": 5.404171466827393,
      "learning_rate": 5.378787878787879e-05,
      "loss": 1.2991,
      "step": 610
    },
    {
      "epoch": 4.696969696969697,
      "grad_norm": 4.330773830413818,
      "learning_rate": 5.303030303030303e-05,
      "loss": 1.1681,
      "step": 620
    },
    {
      "epoch": 4.7727272727272725,
      "grad_norm": 5.318016529083252,
      "learning_rate": 5.2272727272727274e-05,
      "loss": 1.1583,
      "step": 630
    },
    {
      "epoch": 4.848484848484849,
      "grad_norm": 5.8669281005859375,
      "learning_rate": 5.151515151515152e-05,
      "loss": 1.129,
      "step": 640
    },
    {
      "epoch": 4.924242424242424,
      "grad_norm": 4.813358783721924,
      "learning_rate": 5.075757575757576e-05,
      "loss": 1.2659,
      "step": 650
    },
    {
      "epoch": 5.0,
      "grad_norm": 5.2341108322143555,
      "learning_rate": 5e-05,
      "loss": 1.1266,
      "step": 660
    },
    {
      "epoch": 5.075757575757576,
      "grad_norm": 5.485081672668457,
      "learning_rate": 4.9242424242424245e-05,
      "loss": 0.9835,
      "step": 670
    },
    {
      "epoch": 5.151515151515151,
      "grad_norm": 5.523050785064697,
      "learning_rate": 4.848484848484849e-05,
      "loss": 0.918,
      "step": 680
    },
    {
      "epoch": 5.2272727272727275,
      "grad_norm": 6.655966758728027,
      "learning_rate": 4.772727272727273e-05,
      "loss": 1.0007,
      "step": 690
    },
    {
      "epoch": 5.303030303030303,
      "grad_norm": 6.044086933135986,
      "learning_rate": 4.696969696969697e-05,
      "loss": 0.9507,
      "step": 700
    },
    {
      "epoch": 5.378787878787879,
      "grad_norm": 5.638299465179443,
      "learning_rate": 4.621212121212121e-05,
      "loss": 0.9189,
      "step": 710
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 6.3405938148498535,
      "learning_rate": 4.545454545454546e-05,
      "loss": 0.9607,
      "step": 720
    },
    {
      "epoch": 5.53030303030303,
      "grad_norm": 9.555933952331543,
      "learning_rate": 4.46969696969697e-05,
      "loss": 0.9236,
      "step": 730
    },
    {
      "epoch": 5.606060606060606,
      "grad_norm": 5.396800518035889,
      "learning_rate": 4.3939393939393944e-05,
      "loss": 0.9761,
      "step": 740
    },
    {
      "epoch": 5.681818181818182,
      "grad_norm": 5.966156482696533,
      "learning_rate": 4.318181818181819e-05,
      "loss": 0.9929,
      "step": 750
    },
    {
      "epoch": 5.757575757575758,
      "grad_norm": 5.894033432006836,
      "learning_rate": 4.242424242424243e-05,
      "loss": 1.0009,
      "step": 760
    },
    {
      "epoch": 5.833333333333333,
      "grad_norm": 6.456816673278809,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.9845,
      "step": 770
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 7.624484539031982,
      "learning_rate": 4.0909090909090915e-05,
      "loss": 0.9405,
      "step": 780
    },
    {
      "epoch": 5.984848484848484,
      "grad_norm": 7.162961483001709,
      "learning_rate": 4.015151515151515e-05,
      "loss": 0.9375,
      "step": 790
    },
    {
      "epoch": 6.0606060606060606,
      "grad_norm": 9.714057922363281,
      "learning_rate": 3.939393939393939e-05,
      "loss": 0.8267,
      "step": 800
    },
    {
      "epoch": 6.136363636363637,
      "grad_norm": 6.469361305236816,
      "learning_rate": 3.8636363636363636e-05,
      "loss": 0.8483,
      "step": 810
    },
    {
      "epoch": 6.212121212121212,
      "grad_norm": 6.8230390548706055,
      "learning_rate": 3.787878787878788e-05,
      "loss": 0.7375,
      "step": 820
    },
    {
      "epoch": 6.287878787878788,
      "grad_norm": 7.595003128051758,
      "learning_rate": 3.712121212121212e-05,
      "loss": 0.7595,
      "step": 830
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 6.027260780334473,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 0.886,
      "step": 840
    },
    {
      "epoch": 6.4393939393939394,
      "grad_norm": 5.846589088439941,
      "learning_rate": 3.560606060606061e-05,
      "loss": 0.8269,
      "step": 850
    },
    {
      "epoch": 6.515151515151516,
      "grad_norm": 6.768025875091553,
      "learning_rate": 3.484848484848485e-05,
      "loss": 0.7505,
      "step": 860
    },
    {
      "epoch": 6.590909090909091,
      "grad_norm": 6.08740234375,
      "learning_rate": 3.409090909090909e-05,
      "loss": 0.7977,
      "step": 870
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 7.603739261627197,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.7339,
      "step": 880
    },
    {
      "epoch": 6.742424242424242,
      "grad_norm": 6.749317169189453,
      "learning_rate": 3.257575757575758e-05,
      "loss": 0.7562,
      "step": 890
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 8.978140830993652,
      "learning_rate": 3.181818181818182e-05,
      "loss": 0.8161,
      "step": 900
    },
    {
      "epoch": 6.893939393939394,
      "grad_norm": 8.67110538482666,
      "learning_rate": 3.106060606060606e-05,
      "loss": 0.7578,
      "step": 910
    },
    {
      "epoch": 6.96969696969697,
      "grad_norm": 6.555337905883789,
      "learning_rate": 3.0303030303030306e-05,
      "loss": 0.8156,
      "step": 920
    },
    {
      "epoch": 7.045454545454546,
      "grad_norm": 6.72857666015625,
      "learning_rate": 2.954545454545455e-05,
      "loss": 0.7714,
      "step": 930
    },
    {
      "epoch": 7.121212121212121,
      "grad_norm": 7.200118064880371,
      "learning_rate": 2.878787878787879e-05,
      "loss": 0.6132,
      "step": 940
    },
    {
      "epoch": 7.196969696969697,
      "grad_norm": 8.92899227142334,
      "learning_rate": 2.803030303030303e-05,
      "loss": 0.6607,
      "step": 950
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 8.921228408813477,
      "learning_rate": 2.7272727272727273e-05,
      "loss": 0.6984,
      "step": 960
    },
    {
      "epoch": 7.348484848484849,
      "grad_norm": 9.299686431884766,
      "learning_rate": 2.6515151515151516e-05,
      "loss": 0.6078,
      "step": 970
    },
    {
      "epoch": 7.424242424242424,
      "grad_norm": 7.807422161102295,
      "learning_rate": 2.575757575757576e-05,
      "loss": 0.7022,
      "step": 980
    },
    {
      "epoch": 7.5,
      "grad_norm": 7.012553691864014,
      "learning_rate": 2.5e-05,
      "loss": 0.6843,
      "step": 990
    },
    {
      "epoch": 7.575757575757576,
      "grad_norm": 7.715740203857422,
      "learning_rate": 2.4242424242424244e-05,
      "loss": 0.6629,
      "step": 1000
    },
    {
      "epoch": 7.651515151515151,
      "grad_norm": 7.2081708908081055,
      "learning_rate": 2.3484848484848487e-05,
      "loss": 0.5782,
      "step": 1010
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 9.246760368347168,
      "learning_rate": 2.272727272727273e-05,
      "loss": 0.6333,
      "step": 1020
    },
    {
      "epoch": 7.803030303030303,
      "grad_norm": 8.046935081481934,
      "learning_rate": 2.1969696969696972e-05,
      "loss": 0.6168,
      "step": 1030
    },
    {
      "epoch": 7.878787878787879,
      "grad_norm": 5.270540714263916,
      "learning_rate": 2.1212121212121215e-05,
      "loss": 0.6148,
      "step": 1040
    },
    {
      "epoch": 7.954545454545455,
      "grad_norm": 6.898601055145264,
      "learning_rate": 2.0454545454545457e-05,
      "loss": 0.6696,
      "step": 1050
    },
    {
      "epoch": 8.030303030303031,
      "grad_norm": 6.852944374084473,
      "learning_rate": 1.9696969696969697e-05,
      "loss": 0.5936,
      "step": 1060
    },
    {
      "epoch": 8.106060606060606,
      "grad_norm": 7.970870018005371,
      "learning_rate": 1.893939393939394e-05,
      "loss": 0.5444,
      "step": 1070
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 8.113177299499512,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 0.507,
      "step": 1080
    },
    {
      "epoch": 8.257575757575758,
      "grad_norm": 7.545407772064209,
      "learning_rate": 1.7424242424242425e-05,
      "loss": 0.5112,
      "step": 1090
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 8.853331565856934,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.538,
      "step": 1100
    },
    {
      "epoch": 8.409090909090908,
      "grad_norm": 6.817451477050781,
      "learning_rate": 1.590909090909091e-05,
      "loss": 0.6093,
      "step": 1110
    },
    {
      "epoch": 8.484848484848484,
      "grad_norm": 7.042514801025391,
      "learning_rate": 1.5151515151515153e-05,
      "loss": 0.5201,
      "step": 1120
    },
    {
      "epoch": 8.56060606060606,
      "grad_norm": 16.188526153564453,
      "learning_rate": 1.4393939393939396e-05,
      "loss": 0.4961,
      "step": 1130
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 7.460588455200195,
      "learning_rate": 1.3636363636363637e-05,
      "loss": 0.5211,
      "step": 1140
    },
    {
      "epoch": 8.712121212121213,
      "grad_norm": 5.8768439292907715,
      "learning_rate": 1.287878787878788e-05,
      "loss": 0.5807,
      "step": 1150
    },
    {
      "epoch": 8.787878787878787,
      "grad_norm": 6.323732376098633,
      "learning_rate": 1.2121212121212122e-05,
      "loss": 0.5564,
      "step": 1160
    },
    {
      "epoch": 8.863636363636363,
      "grad_norm": 6.50359582901001,
      "learning_rate": 1.1363636363636365e-05,
      "loss": 0.5295,
      "step": 1170
    },
    {
      "epoch": 8.93939393939394,
      "grad_norm": 15.678689956665039,
      "learning_rate": 1.0606060606060607e-05,
      "loss": 0.5178,
      "step": 1180
    },
    {
      "epoch": 9.015151515151516,
      "grad_norm": 8.526084899902344,
      "learning_rate": 9.848484848484848e-06,
      "loss": 0.4938,
      "step": 1190
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 6.922901630401611,
      "learning_rate": 9.090909090909091e-06,
      "loss": 0.445,
      "step": 1200
    },
    {
      "epoch": 9.166666666666666,
      "grad_norm": 6.823384761810303,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.44,
      "step": 1210
    },
    {
      "epoch": 9.242424242424242,
      "grad_norm": 10.130348205566406,
      "learning_rate": 7.5757575757575764e-06,
      "loss": 0.4222,
      "step": 1220
    },
    {
      "epoch": 9.318181818181818,
      "grad_norm": 7.732985019683838,
      "learning_rate": 6.818181818181818e-06,
      "loss": 0.4513,
      "step": 1230
    },
    {
      "epoch": 9.393939393939394,
      "grad_norm": 7.4965691566467285,
      "learning_rate": 6.060606060606061e-06,
      "loss": 0.4564,
      "step": 1240
    },
    {
      "epoch": 9.469696969696969,
      "grad_norm": 6.569212913513184,
      "learning_rate": 5.303030303030304e-06,
      "loss": 0.4637,
      "step": 1250
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 8.648786544799805,
      "learning_rate": 4.5454545454545455e-06,
      "loss": 0.5028,
      "step": 1260
    },
    {
      "epoch": 9.621212121212121,
      "grad_norm": 10.615764617919922,
      "learning_rate": 3.7878787878787882e-06,
      "loss": 0.4492,
      "step": 1270
    },
    {
      "epoch": 9.696969696969697,
      "grad_norm": 7.6753621101379395,
      "learning_rate": 3.0303030303030305e-06,
      "loss": 0.4383,
      "step": 1280
    },
    {
      "epoch": 9.772727272727273,
      "grad_norm": 8.070159912109375,
      "learning_rate": 2.2727272727272728e-06,
      "loss": 0.4291,
      "step": 1290
    },
    {
      "epoch": 9.848484848484848,
      "grad_norm": 7.613664627075195,
      "learning_rate": 1.5151515151515152e-06,
      "loss": 0.5086,
      "step": 1300
    },
    {
      "epoch": 9.924242424242424,
      "grad_norm": 6.221423149108887,
      "learning_rate": 7.575757575757576e-07,
      "loss": 0.4668,
      "step": 1310
    },
    {
      "epoch": 10.0,
      "grad_norm": 5.324963569641113,
      "learning_rate": 0.0,
      "loss": 0.4265,
      "step": 1320
    }
  ],
  "logging_steps": 10,
  "max_steps": 1320,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.762441796847206e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
