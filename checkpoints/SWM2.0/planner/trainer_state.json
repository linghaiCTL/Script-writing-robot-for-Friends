{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 1320,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.07575757575757576,
      "grad_norm": 1.0327720642089844,
      "learning_rate": 9.924242424242425e-05,
      "loss": 2.2408,
      "step": 10
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 0.9391514658927917,
      "learning_rate": 9.848484848484849e-05,
      "loss": 2.1281,
      "step": 20
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 0.9306557178497314,
      "learning_rate": 9.772727272727274e-05,
      "loss": 2.0454,
      "step": 30
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 0.9640492796897888,
      "learning_rate": 9.696969696969698e-05,
      "loss": 1.9816,
      "step": 40
    },
    {
      "epoch": 0.3787878787878788,
      "grad_norm": 0.9821137189865112,
      "learning_rate": 9.621212121212123e-05,
      "loss": 1.9975,
      "step": 50
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 0.9827637672424316,
      "learning_rate": 9.545454545454546e-05,
      "loss": 2.0115,
      "step": 60
    },
    {
      "epoch": 0.5303030303030303,
      "grad_norm": 1.0884462594985962,
      "learning_rate": 9.469696969696971e-05,
      "loss": 2.0006,
      "step": 70
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 1.0326436758041382,
      "learning_rate": 9.393939393939395e-05,
      "loss": 1.9708,
      "step": 80
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 1.0823423862457275,
      "learning_rate": 9.318181818181818e-05,
      "loss": 1.9411,
      "step": 90
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 1.1129875183105469,
      "learning_rate": 9.242424242424242e-05,
      "loss": 1.9497,
      "step": 100
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1.1642963886260986,
      "learning_rate": 9.166666666666667e-05,
      "loss": 1.9581,
      "step": 110
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 1.125937819480896,
      "learning_rate": 9.090909090909092e-05,
      "loss": 1.9731,
      "step": 120
    },
    {
      "epoch": 0.9848484848484849,
      "grad_norm": 1.1041721105575562,
      "learning_rate": 9.015151515151515e-05,
      "loss": 1.9543,
      "step": 130
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 1.015407919883728,
      "learning_rate": 8.93939393939394e-05,
      "loss": 1.8869,
      "step": 140
    },
    {
      "epoch": 1.1363636363636362,
      "grad_norm": 1.215275764465332,
      "learning_rate": 8.863636363636364e-05,
      "loss": 1.8426,
      "step": 150
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 1.3552930355072021,
      "learning_rate": 8.787878787878789e-05,
      "loss": 1.8732,
      "step": 160
    },
    {
      "epoch": 1.2878787878787878,
      "grad_norm": 1.3572676181793213,
      "learning_rate": 8.712121212121212e-05,
      "loss": 1.8377,
      "step": 170
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 1.3581441640853882,
      "learning_rate": 8.636363636363637e-05,
      "loss": 1.8544,
      "step": 180
    },
    {
      "epoch": 1.4393939393939394,
      "grad_norm": 1.405535340309143,
      "learning_rate": 8.560606060606061e-05,
      "loss": 1.8241,
      "step": 190
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 1.4101777076721191,
      "learning_rate": 8.484848484848486e-05,
      "loss": 1.8556,
      "step": 200
    },
    {
      "epoch": 1.5909090909090908,
      "grad_norm": 1.531159520149231,
      "learning_rate": 8.40909090909091e-05,
      "loss": 1.833,
      "step": 210
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.555901288986206,
      "learning_rate": 8.333333333333334e-05,
      "loss": 1.8306,
      "step": 220
    },
    {
      "epoch": 1.7424242424242424,
      "grad_norm": 1.5169209241867065,
      "learning_rate": 8.257575757575758e-05,
      "loss": 1.839,
      "step": 230
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 1.5520961284637451,
      "learning_rate": 8.181818181818183e-05,
      "loss": 1.7913,
      "step": 240
    },
    {
      "epoch": 1.893939393939394,
      "grad_norm": 1.503513216972351,
      "learning_rate": 8.106060606060607e-05,
      "loss": 1.8182,
      "step": 250
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 1.4520834684371948,
      "learning_rate": 8.03030303030303e-05,
      "loss": 1.849,
      "step": 260
    },
    {
      "epoch": 2.0454545454545454,
      "grad_norm": 1.5599576234817505,
      "learning_rate": 7.954545454545455e-05,
      "loss": 1.7598,
      "step": 270
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 1.5634552240371704,
      "learning_rate": 7.878787878787879e-05,
      "loss": 1.7312,
      "step": 280
    },
    {
      "epoch": 2.196969696969697,
      "grad_norm": 1.945818305015564,
      "learning_rate": 7.803030303030304e-05,
      "loss": 1.6992,
      "step": 290
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 1.7949706315994263,
      "learning_rate": 7.727272727272727e-05,
      "loss": 1.7274,
      "step": 300
    },
    {
      "epoch": 2.3484848484848486,
      "grad_norm": 2.0288949012756348,
      "learning_rate": 7.651515151515152e-05,
      "loss": 1.7105,
      "step": 310
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 1.888155221939087,
      "learning_rate": 7.575757575757576e-05,
      "loss": 1.7457,
      "step": 320
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.8706738948822021,
      "learning_rate": 7.500000000000001e-05,
      "loss": 1.7366,
      "step": 330
    },
    {
      "epoch": 2.5757575757575757,
      "grad_norm": 2.042722463607788,
      "learning_rate": 7.424242424242424e-05,
      "loss": 1.6584,
      "step": 340
    },
    {
      "epoch": 2.6515151515151514,
      "grad_norm": 2.015690326690674,
      "learning_rate": 7.348484848484849e-05,
      "loss": 1.6949,
      "step": 350
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 2.138148307800293,
      "learning_rate": 7.272727272727273e-05,
      "loss": 1.6662,
      "step": 360
    },
    {
      "epoch": 2.8030303030303028,
      "grad_norm": 1.9213534593582153,
      "learning_rate": 7.196969696969698e-05,
      "loss": 1.6958,
      "step": 370
    },
    {
      "epoch": 2.878787878787879,
      "grad_norm": 1.9121942520141602,
      "learning_rate": 7.121212121212121e-05,
      "loss": 1.6787,
      "step": 380
    },
    {
      "epoch": 2.9545454545454546,
      "grad_norm": 2.0546414852142334,
      "learning_rate": 7.045454545454546e-05,
      "loss": 1.7005,
      "step": 390
    },
    {
      "epoch": 3.0303030303030303,
      "grad_norm": 1.9160171747207642,
      "learning_rate": 6.96969696969697e-05,
      "loss": 1.6621,
      "step": 400
    },
    {
      "epoch": 3.106060606060606,
      "grad_norm": 2.256882429122925,
      "learning_rate": 6.893939393939395e-05,
      "loss": 1.5895,
      "step": 410
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 2.135338306427002,
      "learning_rate": 6.818181818181818e-05,
      "loss": 1.5571,
      "step": 420
    },
    {
      "epoch": 3.257575757575758,
      "grad_norm": 2.239332437515259,
      "learning_rate": 6.742424242424242e-05,
      "loss": 1.59,
      "step": 430
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 2.5733439922332764,
      "learning_rate": 6.666666666666667e-05,
      "loss": 1.6022,
      "step": 440
    },
    {
      "epoch": 3.409090909090909,
      "grad_norm": 2.2938432693481445,
      "learning_rate": 6.59090909090909e-05,
      "loss": 1.5792,
      "step": 450
    },
    {
      "epoch": 3.484848484848485,
      "grad_norm": 2.337425947189331,
      "learning_rate": 6.515151515151516e-05,
      "loss": 1.5866,
      "step": 460
    },
    {
      "epoch": 3.5606060606060606,
      "grad_norm": 2.4033546447753906,
      "learning_rate": 6.439393939393939e-05,
      "loss": 1.5965,
      "step": 470
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 2.378504753112793,
      "learning_rate": 6.363636363636364e-05,
      "loss": 1.597,
      "step": 480
    },
    {
      "epoch": 3.712121212121212,
      "grad_norm": 2.3378360271453857,
      "learning_rate": 6.287878787878788e-05,
      "loss": 1.6055,
      "step": 490
    },
    {
      "epoch": 3.787878787878788,
      "grad_norm": 2.5083625316619873,
      "learning_rate": 6.212121212121213e-05,
      "loss": 1.6001,
      "step": 500
    },
    {
      "epoch": 3.8636363636363638,
      "grad_norm": 2.363926649093628,
      "learning_rate": 6.136363636363636e-05,
      "loss": 1.5733,
      "step": 510
    },
    {
      "epoch": 3.9393939393939394,
      "grad_norm": 2.2771966457366943,
      "learning_rate": 6.060606060606061e-05,
      "loss": 1.559,
      "step": 520
    },
    {
      "epoch": 4.015151515151516,
      "grad_norm": 2.275099515914917,
      "learning_rate": 5.9848484848484854e-05,
      "loss": 1.5457,
      "step": 530
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 2.4492523670196533,
      "learning_rate": 5.90909090909091e-05,
      "loss": 1.4411,
      "step": 540
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 2.4735960960388184,
      "learning_rate": 5.833333333333334e-05,
      "loss": 1.4453,
      "step": 550
    },
    {
      "epoch": 4.242424242424242,
      "grad_norm": 2.7071692943573,
      "learning_rate": 5.757575757575758e-05,
      "loss": 1.4635,
      "step": 560
    },
    {
      "epoch": 4.318181818181818,
      "grad_norm": 2.900683879852295,
      "learning_rate": 5.6818181818181825e-05,
      "loss": 1.4704,
      "step": 570
    },
    {
      "epoch": 4.393939393939394,
      "grad_norm": 2.575950860977173,
      "learning_rate": 5.606060606060606e-05,
      "loss": 1.4793,
      "step": 580
    },
    {
      "epoch": 4.46969696969697,
      "grad_norm": 2.7565603256225586,
      "learning_rate": 5.5303030303030304e-05,
      "loss": 1.4555,
      "step": 590
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 2.9325902462005615,
      "learning_rate": 5.4545454545454546e-05,
      "loss": 1.4698,
      "step": 600
    },
    {
      "epoch": 4.621212121212121,
      "grad_norm": 2.7423171997070312,
      "learning_rate": 5.378787878787879e-05,
      "loss": 1.4502,
      "step": 610
    },
    {
      "epoch": 4.696969696969697,
      "grad_norm": 2.780569553375244,
      "learning_rate": 5.303030303030303e-05,
      "loss": 1.4694,
      "step": 620
    },
    {
      "epoch": 4.7727272727272725,
      "grad_norm": 2.745211601257324,
      "learning_rate": 5.2272727272727274e-05,
      "loss": 1.5009,
      "step": 630
    },
    {
      "epoch": 4.848484848484849,
      "grad_norm": 2.791011095046997,
      "learning_rate": 5.151515151515152e-05,
      "loss": 1.4662,
      "step": 640
    },
    {
      "epoch": 4.924242424242424,
      "grad_norm": 2.819481372833252,
      "learning_rate": 5.075757575757576e-05,
      "loss": 1.5058,
      "step": 650
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.9141175746917725,
      "learning_rate": 5e-05,
      "loss": 1.4793,
      "step": 660
    },
    {
      "epoch": 5.075757575757576,
      "grad_norm": 3.161123514175415,
      "learning_rate": 4.9242424242424245e-05,
      "loss": 1.3499,
      "step": 670
    },
    {
      "epoch": 5.151515151515151,
      "grad_norm": 3.1757326126098633,
      "learning_rate": 4.848484848484849e-05,
      "loss": 1.3148,
      "step": 680
    },
    {
      "epoch": 5.2272727272727275,
      "grad_norm": 3.043623447418213,
      "learning_rate": 4.772727272727273e-05,
      "loss": 1.3386,
      "step": 690
    },
    {
      "epoch": 5.303030303030303,
      "grad_norm": 3.447153091430664,
      "learning_rate": 4.696969696969697e-05,
      "loss": 1.3699,
      "step": 700
    },
    {
      "epoch": 5.378787878787879,
      "grad_norm": 3.5898284912109375,
      "learning_rate": 4.621212121212121e-05,
      "loss": 1.3687,
      "step": 710
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 3.0795202255249023,
      "learning_rate": 4.545454545454546e-05,
      "loss": 1.3299,
      "step": 720
    },
    {
      "epoch": 5.53030303030303,
      "grad_norm": 3.231618881225586,
      "learning_rate": 4.46969696969697e-05,
      "loss": 1.3984,
      "step": 730
    },
    {
      "epoch": 5.606060606060606,
      "grad_norm": 3.2524938583374023,
      "learning_rate": 4.3939393939393944e-05,
      "loss": 1.3576,
      "step": 740
    },
    {
      "epoch": 5.681818181818182,
      "grad_norm": 3.275221109390259,
      "learning_rate": 4.318181818181819e-05,
      "loss": 1.3591,
      "step": 750
    },
    {
      "epoch": 5.757575757575758,
      "grad_norm": 3.4410386085510254,
      "learning_rate": 4.242424242424243e-05,
      "loss": 1.3259,
      "step": 760
    },
    {
      "epoch": 5.833333333333333,
      "grad_norm": 3.336515188217163,
      "learning_rate": 4.166666666666667e-05,
      "loss": 1.3962,
      "step": 770
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 3.0830464363098145,
      "learning_rate": 4.0909090909090915e-05,
      "loss": 1.3638,
      "step": 780
    },
    {
      "epoch": 5.984848484848484,
      "grad_norm": 3.069225549697876,
      "learning_rate": 4.015151515151515e-05,
      "loss": 1.3887,
      "step": 790
    },
    {
      "epoch": 6.0606060606060606,
      "grad_norm": 3.142338991165161,
      "learning_rate": 3.939393939393939e-05,
      "loss": 1.2903,
      "step": 800
    },
    {
      "epoch": 6.136363636363637,
      "grad_norm": 3.7902932167053223,
      "learning_rate": 3.8636363636363636e-05,
      "loss": 1.2528,
      "step": 810
    },
    {
      "epoch": 6.212121212121212,
      "grad_norm": 3.4186999797821045,
      "learning_rate": 3.787878787878788e-05,
      "loss": 1.2527,
      "step": 820
    },
    {
      "epoch": 6.287878787878788,
      "grad_norm": 3.4179751873016357,
      "learning_rate": 3.712121212121212e-05,
      "loss": 1.2429,
      "step": 830
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 3.984349012374878,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 1.2434,
      "step": 840
    },
    {
      "epoch": 6.4393939393939394,
      "grad_norm": 3.4980454444885254,
      "learning_rate": 3.560606060606061e-05,
      "loss": 1.2923,
      "step": 850
    },
    {
      "epoch": 6.515151515151516,
      "grad_norm": 3.652204990386963,
      "learning_rate": 3.484848484848485e-05,
      "loss": 1.2862,
      "step": 860
    },
    {
      "epoch": 6.590909090909091,
      "grad_norm": 3.543947696685791,
      "learning_rate": 3.409090909090909e-05,
      "loss": 1.2456,
      "step": 870
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 3.5518639087677,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 1.2373,
      "step": 880
    },
    {
      "epoch": 6.742424242424242,
      "grad_norm": 3.4782238006591797,
      "learning_rate": 3.257575757575758e-05,
      "loss": 1.2765,
      "step": 890
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 3.4058144092559814,
      "learning_rate": 3.181818181818182e-05,
      "loss": 1.2346,
      "step": 900
    },
    {
      "epoch": 6.893939393939394,
      "grad_norm": 3.861118793487549,
      "learning_rate": 3.106060606060606e-05,
      "loss": 1.2683,
      "step": 910
    },
    {
      "epoch": 6.96969696969697,
      "grad_norm": 3.627480983734131,
      "learning_rate": 3.0303030303030306e-05,
      "loss": 1.305,
      "step": 920
    },
    {
      "epoch": 7.045454545454546,
      "grad_norm": 3.6667096614837646,
      "learning_rate": 2.954545454545455e-05,
      "loss": 1.2407,
      "step": 930
    },
    {
      "epoch": 7.121212121212121,
      "grad_norm": 4.097111701965332,
      "learning_rate": 2.878787878787879e-05,
      "loss": 1.1593,
      "step": 940
    },
    {
      "epoch": 7.196969696969697,
      "grad_norm": 3.5977203845977783,
      "learning_rate": 2.803030303030303e-05,
      "loss": 1.1706,
      "step": 950
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 3.8965024948120117,
      "learning_rate": 2.7272727272727273e-05,
      "loss": 1.141,
      "step": 960
    },
    {
      "epoch": 7.348484848484849,
      "grad_norm": 3.6665749549865723,
      "learning_rate": 2.6515151515151516e-05,
      "loss": 1.1623,
      "step": 970
    },
    {
      "epoch": 7.424242424242424,
      "grad_norm": 4.017276763916016,
      "learning_rate": 2.575757575757576e-05,
      "loss": 1.2023,
      "step": 980
    },
    {
      "epoch": 7.5,
      "grad_norm": 3.7551636695861816,
      "learning_rate": 2.5e-05,
      "loss": 1.1977,
      "step": 990
    },
    {
      "epoch": 7.575757575757576,
      "grad_norm": 3.6861844062805176,
      "learning_rate": 2.4242424242424244e-05,
      "loss": 1.1696,
      "step": 1000
    },
    {
      "epoch": 7.651515151515151,
      "grad_norm": 3.8869011402130127,
      "learning_rate": 2.3484848484848487e-05,
      "loss": 1.161,
      "step": 1010
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 4.281241416931152,
      "learning_rate": 2.272727272727273e-05,
      "loss": 1.1979,
      "step": 1020
    },
    {
      "epoch": 7.803030303030303,
      "grad_norm": 3.810039520263672,
      "learning_rate": 2.1969696969696972e-05,
      "loss": 1.1976,
      "step": 1030
    },
    {
      "epoch": 7.878787878787879,
      "grad_norm": 4.008768558502197,
      "learning_rate": 2.1212121212121215e-05,
      "loss": 1.1588,
      "step": 1040
    },
    {
      "epoch": 7.954545454545455,
      "grad_norm": 4.09407901763916,
      "learning_rate": 2.0454545454545457e-05,
      "loss": 1.1678,
      "step": 1050
    },
    {
      "epoch": 8.030303030303031,
      "grad_norm": 3.6334424018859863,
      "learning_rate": 1.9696969696969697e-05,
      "loss": 1.1659,
      "step": 1060
    },
    {
      "epoch": 8.106060606060606,
      "grad_norm": 3.9906158447265625,
      "learning_rate": 1.893939393939394e-05,
      "loss": 1.1318,
      "step": 1070
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 3.8549892902374268,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 1.1139,
      "step": 1080
    },
    {
      "epoch": 8.257575757575758,
      "grad_norm": 3.861506462097168,
      "learning_rate": 1.7424242424242425e-05,
      "loss": 1.1349,
      "step": 1090
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 3.845366954803467,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 1.1208,
      "step": 1100
    },
    {
      "epoch": 8.409090909090908,
      "grad_norm": 4.360313415527344,
      "learning_rate": 1.590909090909091e-05,
      "loss": 1.0863,
      "step": 1110
    },
    {
      "epoch": 8.484848484848484,
      "grad_norm": 3.8889822959899902,
      "learning_rate": 1.5151515151515153e-05,
      "loss": 1.1156,
      "step": 1120
    },
    {
      "epoch": 8.56060606060606,
      "grad_norm": 4.160747528076172,
      "learning_rate": 1.4393939393939396e-05,
      "loss": 1.0982,
      "step": 1130
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 4.1006927490234375,
      "learning_rate": 1.3636363636363637e-05,
      "loss": 1.0902,
      "step": 1140
    },
    {
      "epoch": 8.712121212121213,
      "grad_norm": 4.189971446990967,
      "learning_rate": 1.287878787878788e-05,
      "loss": 1.0981,
      "step": 1150
    },
    {
      "epoch": 8.787878787878787,
      "grad_norm": 4.04060697555542,
      "learning_rate": 1.2121212121212122e-05,
      "loss": 1.0886,
      "step": 1160
    },
    {
      "epoch": 8.863636363636363,
      "grad_norm": 4.298552513122559,
      "learning_rate": 1.1363636363636365e-05,
      "loss": 1.1021,
      "step": 1170
    },
    {
      "epoch": 8.93939393939394,
      "grad_norm": 3.8528923988342285,
      "learning_rate": 1.0606060606060607e-05,
      "loss": 1.103,
      "step": 1180
    },
    {
      "epoch": 9.015151515151516,
      "grad_norm": 3.7824900150299072,
      "learning_rate": 9.848484848484848e-06,
      "loss": 1.1166,
      "step": 1190
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 4.16897439956665,
      "learning_rate": 9.090909090909091e-06,
      "loss": 1.0243,
      "step": 1200
    },
    {
      "epoch": 9.166666666666666,
      "grad_norm": 4.290210247039795,
      "learning_rate": 8.333333333333334e-06,
      "loss": 1.0835,
      "step": 1210
    },
    {
      "epoch": 9.242424242424242,
      "grad_norm": 4.348971366882324,
      "learning_rate": 7.5757575757575764e-06,
      "loss": 1.0569,
      "step": 1220
    },
    {
      "epoch": 9.318181818181818,
      "grad_norm": 4.235450744628906,
      "learning_rate": 6.818181818181818e-06,
      "loss": 1.056,
      "step": 1230
    },
    {
      "epoch": 9.393939393939394,
      "grad_norm": 3.9932777881622314,
      "learning_rate": 6.060606060606061e-06,
      "loss": 1.0595,
      "step": 1240
    },
    {
      "epoch": 9.469696969696969,
      "grad_norm": 4.261719226837158,
      "learning_rate": 5.303030303030304e-06,
      "loss": 1.0515,
      "step": 1250
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 4.340458869934082,
      "learning_rate": 4.5454545454545455e-06,
      "loss": 1.078,
      "step": 1260
    },
    {
      "epoch": 9.621212121212121,
      "grad_norm": 4.680603504180908,
      "learning_rate": 3.7878787878787882e-06,
      "loss": 1.0552,
      "step": 1270
    },
    {
      "epoch": 9.696969696969697,
      "grad_norm": 4.609933376312256,
      "learning_rate": 3.0303030303030305e-06,
      "loss": 1.0314,
      "step": 1280
    },
    {
      "epoch": 9.772727272727273,
      "grad_norm": 4.308242321014404,
      "learning_rate": 2.2727272727272728e-06,
      "loss": 1.0587,
      "step": 1290
    },
    {
      "epoch": 9.848484848484848,
      "grad_norm": 4.124973773956299,
      "learning_rate": 1.5151515151515152e-06,
      "loss": 1.0666,
      "step": 1300
    },
    {
      "epoch": 9.924242424242424,
      "grad_norm": 4.492101192474365,
      "learning_rate": 7.575757575757576e-07,
      "loss": 1.0665,
      "step": 1310
    },
    {
      "epoch": 10.0,
      "grad_norm": 4.278134822845459,
      "learning_rate": 0.0,
      "loss": 1.0477,
      "step": 1320
    }
  ],
  "logging_steps": 10,
  "max_steps": 1320,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.613148680981709e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
